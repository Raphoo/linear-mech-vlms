{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eff17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f6cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Figure 8a: Ground Truth Deviation Analysis\n",
    "MODELS_CONFIG = {\n",
    "    \"llama-11b\": {\"layer\": 14, \"data_dir\": \"embeds/coco_deviation/llama-11b\"},\n",
    "    \"llava-7b\": {\"layer\": 12, \"data_dir\": \"embeds/coco_deviation/llava-7b\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb54fa1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1175a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.extract_embeds import VLMForExtraction\n",
    "\n",
    "# We'll load the tokenizer for each model in the loop below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1bddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from pycocotools.coco import COCO\n",
    "from utils.coco import load_coco_annotations, get_subject_spatial_id\n",
    "from utils.linalg import project_onto_plane, euclidean_distances\n",
    "import json\n",
    "\n",
    "# Load COCO annotations once\n",
    "coco = load_coco_annotations(\"data/coco/annotations/instances_val2017.json\")\n",
    "\n",
    "def verdict(sequences, pos_word, neg_word, tokenizer):\n",
    "    response = tokenizer.decode(sequences[0]).split(\"assistant\")[-1].split(\"ASSISTANT\")[-1]\n",
    "    if pos_word.lower() in response.lower() and neg_word.lower() not in response.lower():\n",
    "        return True\n",
    "    elif neg_word.lower() in response.lower() and pos_word.lower() not in response.lower():\n",
    "        return False\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def mann_whitney_report(blue, red):\n",
    "    blue = np.asarray(blue)\n",
    "    red = np.asarray(red)\n",
    "    # two-sided test\n",
    "    U_two, p_two = stats.mannwhitneyu(blue, red, alternative=\"two-sided\", method=\"auto\")\n",
    "    # one-sided: blue > red\n",
    "    U_greater, p_greater = stats.mannwhitneyu(blue, red, alternative=\"greater\", method=\"auto\")\n",
    "    # one-sided: blue < red\n",
    "    U_less, p_less = stats.mannwhitneyu(blue, red, alternative=\"less\", method=\"auto\")\n",
    "    print(f\"Mann–Whitney U (two-sided): U = {U_two}, p = {p_two:.3g}\")\n",
    "    print(f\"Mann–Whitney U (blue > red): U = {U_greater}, p = {p_greater:.3g}\")\n",
    "    print(f\"Mann–Whitney U (blue < red): U = {U_less}, p = {p_less:.3g}\")\n",
    "    return {\n",
    "        \"U_two\": U_two, \"p_two\": p_two,\n",
    "        \"U_greater\": U_greater, \"p_greater\": p_greater,\n",
    "        \"U_less\": U_less, \"p_less\": p_less,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a595483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(model_name, data_dir, layer, tokenizer):\n",
    "    \"\"\"Run ground truth deviation analysis for Figure 8a\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing {model_name} at layer {layer}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    universal_id = torch.load(f\"embeds/universal_id/{model_name}.pt\")\n",
    "    x_axis = torch.load(f\"embeds/universal_id/{model_name}_x.pt\")[layer][\"universal\"].to(torch.float32).numpy()\n",
    "    y_axis = torch.load(f\"embeds/universal_id/{model_name}_y.pt\")[layer][\"universal\"].to(torch.float32).numpy()\n",
    "    pos_distances = []\n",
    "    neg_distances = []\n",
    "    scores = []\n",
    "    \n",
    "    from matplotlib import pyplot as plt\n",
    "    total = len(os.listdir(data_dir))\n",
    "    correct = 0\n",
    "    bad = []\n",
    "    log = []\n",
    "    \n",
    "    for fname in os.listdir(data_dir):\n",
    "        object_a, object_b, pos_word, neg_word, id = fname.split(\"_\")\n",
    "        object_a, object_b = object_a.replace(\"+\", \" \"), object_b.replace(\"+\", \" \")\n",
    "        id = int(id)\n",
    "        token_a, token_b = [\n",
    "            tokenizer.tokenize(\" \" + tw)[-1] for tw in [object_a, object_b]\n",
    "        ]\n",
    "        \n",
    "        distances = []\n",
    "        x_loc = []\n",
    "        y_loc = []\n",
    "        gt_x_loc = []\n",
    "        gt_y_loc = []\n",
    "        \n",
    "        for object, token, color in [(object_a, token_a, \"red\"), (object_b, token_b, \"blue\")]:\n",
    "            _, gt_embeds = get_subject_spatial_id(id, object, coco, universal_id[layer][\"universal\"])\n",
    "            gt_embeds = gt_embeds.to(torch.float32).numpy()[np.newaxis,:]\n",
    "            text_embeds = torch.load(path.join(data_dir, fname, \"text.pt\"))[layer][token].to(torch.float32).numpy()[np.newaxis,:]\n",
    "            embeds = torch.load(path.join(data_dir, fname, \"embeds.pt\"))[layer][token].to(torch.float32).numpy() - text_embeds\n",
    "            \n",
    "            gt_coords, _ = project_onto_plane(gt_embeds, x_axis, y_axis)\n",
    "            coords, _ = project_onto_plane(embeds, x_axis, y_axis)\n",
    "            \n",
    "            x_loc.append(coords[0, 0])\n",
    "            gt_x_loc.append(gt_coords[0, 0])\n",
    "            y_loc.append(coords[0, 1])\n",
    "            gt_y_loc.append(gt_coords[0, 1])\n",
    "            \n",
    "        if pos_word == \"right\":\n",
    "            distances.append((x_loc[0] - x_loc[1]).item() - (gt_x_loc[0] - gt_x_loc[1]).item())\n",
    "        elif pos_word == \"left\":\n",
    "            distances.append((x_loc[1] - x_loc[0]).item() - (gt_x_loc[1] - gt_x_loc[0]).item())\n",
    "        elif pos_word == \"below\":\n",
    "            distances.append((y_loc[0] - y_loc[1]).item() - (gt_y_loc[0] - gt_y_loc[1]).item())\n",
    "        elif pos_word == \"above\":\n",
    "            distances.append((y_loc[1] - y_loc[0]).item() - (gt_y_loc[1] - gt_y_loc[0]).item())\n",
    "\n",
    "        sequences = torch.load(path.join(data_dir, fname, f\"sequences.pt\"), weights_only=False)\n",
    "        v = verdict(sequences, pos_word, neg_word, tokenizer)\n",
    "\n",
    "        if v is None:\n",
    "            continue\n",
    "        \n",
    "        scores.append((distances[0], id))\n",
    "        if v:\n",
    "            correct += 1\n",
    "            pos_distances.extend(distances)\n",
    "            log.append({\"id\": id, \"distance\": distances[0], \"verdict\": True})\n",
    "        else:\n",
    "            bad.append(id)\n",
    "            neg_distances.extend(distances)\n",
    "            log.append({\"id\": id, \"distance\": distances[0], \"verdict\": False})\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Accuracy: {correct/total:.2%}\")\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.hist(pos_distances, bins=10, alpha=0.7, density=True, color='steelblue', label='Positive')\n",
    "    plt.hist(neg_distances, bins=10, alpha=0.7, density=True, color='salmon', label='Negative')\n",
    "    plt.xlabel('Margin Relative to GT')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'{model_name} Layer {layer}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical test\n",
    "    print(mann_whitney_report(pos_distances, neg_distances))\n",
    "\n",
    "    # Save metadata for Figure 8b\n",
    "    os.makedirs(\"metadata\", exist_ok=True)\n",
    "    output_path = f\"metadata/{model_name}_{layer}.json\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(log, f, indent=2)\n",
    "    print(f\"\\nSaved metadata to {output_path}\")\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de086e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis for both models (Figure 8a)\n",
    "for model_name, config in MODELS_CONFIG.items():\n",
    "    # Load tokenizer for this model\n",
    "    model_str = VLMForExtraction.get_model_string(model_name)\n",
    "    tokenizer = AutoProcessor.from_pretrained(model_str).tokenizer\n",
    "    \n",
    "    # Run pipeline\n",
    "    pipeline(\n",
    "        model_name=model_name,\n",
    "        data_dir=config[\"data_dir\"],\n",
    "        layer=config[\"layer\"],\n",
    "        tokenizer=tokenizer\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm-bind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
