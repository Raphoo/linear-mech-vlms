{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3207e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from utils.linalg import project_onto_plane\n",
    "from utils.plotting import bivariate_color_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 224\n",
    "INTERESTING_LAYERS = list(range(8, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9347f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_onto_computed_xy(embeds, x_axes, y_axes, title, axes_token, token, layer, extra_tokens=None, grid_wh=4):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6.2))\n",
    "\n",
    "    # Collect embeddings for the main token\n",
    "    color = []\n",
    "    selected_embeds = []\n",
    "    for (x, y, size), embed in embeds[layer][token].items():\n",
    "        if size == SIZE:\n",
    "            color.append(bivariate_color_map(x, y, grid_wh, grid_wh))\n",
    "            selected_embeds.append(embed)\n",
    "\n",
    "    # Convert to numpy and project main token embeddings\n",
    "    main_array = torch.stack(selected_embeds).to(torch.float32).numpy()\n",
    "    x_axis = x_axes[layer][axes_token].to(torch.float32).numpy()\n",
    "    y_axis = y_axes[layer][axes_token].to(torch.float32).numpy()\n",
    "    coords_main, explained = project_onto_plane(main_array, x_axis, y_axis)\n",
    "\n",
    "    # Plot main token points\n",
    "    ax.scatter(coords_main[:, 0], coords_main[:, 1], c=np.array(color), marker=\"o\", label=token, s=150)\n",
    "\n",
    "    if extra_tokens is not None:\n",
    "        for label, val in extra_tokens.items():\n",
    "            if isinstance(val, str):\n",
    "                token_embeds = []\n",
    "                for (_, _, size_), embed_val in embeds[layer].get(val, {}).items():\n",
    "                    if size_ == SIZE:\n",
    "                        token_embeds.append(embed_val)\n",
    "                if len(token_embeds) == 0:\n",
    "                    raise ValueError(f\"No embeddings found for extra token '{val}' at layer {layer}.\")\n",
    "                avg_embed = torch.stack(token_embeds).to(torch.float32).mean(dim=0).numpy()\n",
    "                print(avg_embed)\n",
    "            else:\n",
    "                # Assume val is a tensor or array\n",
    "                if isinstance(val, torch.Tensor):\n",
    "                    avg_embed = val[layer].to(torch.float32).cpu().numpy()\n",
    "                else:\n",
    "                    avg_embed = np.asarray(val[layer], dtype=np.float32)\n",
    "\n",
    "            # Project using the shared project_onto_plane function\n",
    "            coords_extra, _ = project_onto_plane(avg_embed[np.newaxis, :], x_axis, y_axis)\n",
    "            coord = coords_extra[0]\n",
    "            ax.scatter(coord[0], coord[1], marker=f\"${label}$\", c=\"black\", s=150)\n",
    "\n",
    "    # Finalize plot\n",
    "    subtitle = f\"Explained: {explained:.2f}\"\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_title(subtitle)\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    fig.suptitle(f\"{title}: Layer {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grid(model, frog_token, left_token, right_token, layer):\n",
    "    embeds = torch.load(f\"embeds/id_grid/camel_frog/{model}.pt\", map_location=\"cpu\",weights_only=True)\n",
    "    x_axes = torch.load(f\"embeds/id_grid/camel_frog/{model}_x.pt\", map_location=\"cpu\",weights_only=True)\n",
    "    y_axes = torch.load(f\"embeds/id_grid/camel_frog/{model}_y.pt\", map_location=\"cpu\",weights_only=True)\n",
    "\n",
    "    lr_embed = torch.load(f\"embeds/id_grid/camel_frog_text/{model}.pt\", map_location=\"cpu\",weights_only=True)\n",
    "    # Use .float() to cast to float32 before converting to numpy\n",
    "    left_embed = np.array([lr_embed[layer][left_token].float().numpy() for layer in lr_embed.keys()])\n",
    "    right_embed = np.array([lr_embed[layer][right_token].float().numpy() for layer in lr_embed.keys()])\n",
    "\n",
    "    project_onto_computed_xy(embeds, x_axes, y_axes, axes_token=frog_token, title=\"Avg Frog Across Varying Reference\", token=frog_token, extra_tokens={\"L\": left_embed, \"R\": right_embed}, layer=layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebf9ab",
   "metadata": {},
   "source": [
    "## Llava-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a47c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_grid(\"llava-7b\", \"rog\", \"▁left\", \"▁right\", layer=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4e564",
   "metadata": {},
   "source": [
    "## Llama-11b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_grid(\"llama-11b\", \"Ġfrog\", \"Ġleft\", \"Ġright\", layer=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm-bind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
